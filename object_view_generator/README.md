
This package contains a service providing node that generates view poses around an object/region/arbitrary point of interest. 
The poses it generates are for the robot base, no PTU angles are provided - these are either easily calculated 
separately to have the PTU point at the target, or the 
[ptu_follow_frame](https://github.com/strands-project/scitos_apps/tree/indigo-devel/ptu_follow_frame)
package can be used in conjunction with the
[static_transform_manager](https://github.com/strands-project/strands_apps/tree/indigo-devel/static_transform_manager)
package to have the PTU always looking at the target point. The generated poses are collision checked against a 2D map 
to be reasonably sure that the robot is capable of navigating to those positions. In the STRANDS work using this code
we use a down projected 3D map (generated by the 
[semantic_map_to_2d](https://github.com/strands-project/strands_3d_mapping/tree/hydro-devel/semantic_map_to_2d)
package) to effectively collision check in 3D.

The node provides two methods of generating the target points, one in which the aim is to generate a sequence of 
views - i.e a robot trajectory, and the other is just to generate a set of view points. The later method is the initial step of
the first method, details are available in !!RA-L paper!!. The method to use is selected via a parameter in the service call.

# Running
start the service node with

`rosrun object_view_generator view_points_service.py` 

This will start the service with the default parameter values, namely:

- `map_topic` : default `'/map'`  - This specifies the topic to pull the map from for testing candidate poses for collision. The topic type must be nav_msgs/OccupancyGrid, and a fresh copy of the map will be waited for at each service call.
- `is_costmap` : default `False`  - This specifies the map type, as costmaps usually have different value meanings than standard maps. This allows the inflated costmap to be used in place of the static 2D map if desired.

# Usage

Once started the node will provide a single service `/generate_object_views` with the following request fields:

- `geometry_msgs/Pose target_pose` - The pose of the target to generate views for. Currently only the position is used, the orientation can be left default.
- `float32 min_dist`  - This is the minimum distance that the view poses should be from the tartget.
- `float32 max_dist` - The maximum distance that the view poses should be from the object.
- `int16 number_views` - How many views (maximum) should be generated. If some of the views are not valid then less than this number might be returned.
- `float32 inflation_radius` - How much space should the robot footprint be expanded by when collision checking the candidate poses.
- `bool return_as_trajectory` - This selects the method for generating the views. If `True`, then only a smaller subset of connected views will be returned, thereby returning a trajectory. If `False` then a larger set of all possible views is provided, but the path between these views is not considered when selecting them.
- `string SOMA_region` - This optional parameter, if supplied, will cause the generator to restrict views to the specified SOMA region. If specified then the format must be "soma_map/soma_config/region_number" - the parsing of this might be sensitive :-). If you supply an empty string then SOMA will not be required on your system.

The return of the service is as follows:

- `geometry_msgs/PoseArray goals` - This is simply the list of goals.

For convenience the service handle will also publish the generated goals on the topic `/object_view_goals` of type `PoseArray`.

# Testing

The test client can be started with 

`rosrun object_view_generator test_with_rviz.py`

This provides a simple text interface to call the service with different parameters. Subscribe to `/object_view_goals` in RViz to see the results.
